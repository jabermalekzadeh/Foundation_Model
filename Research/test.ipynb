{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafdfa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koorosh/Documents/Coding/Foundation_Model/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model to divice\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "local_model_path = \"llama2-7b-chat\"  # or full path if you prefer\n",
    "current_path = os.getcwd()\n",
    "parent_path = os.path.dirname(current_path)\n",
    "path = os.path.join(parent_path, local_model_path)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load tokenizer from local dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    dtype = torch.float16,\n",
    "    local_files_only = True\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "print('model to divice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d56b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell what is optoacosutic.\n",
      "Optoacoustic refers to the use of light (opto) to generate sound (acoustic) for imaging or sensing applications. Optoacoustic techniques use light to excite molecules or particles in a sample, which then emit sound waves that can be detected and used to create images or measure properties of the sample. This approach offers several advantages over traditional imaging methods, including higher resolution, deeper penetration, and non-invasive imaging.\n",
      "\n",
      "In optoacoustic imaging, light is directed at a sample, and the absorption or scattering of the light by the sample causes the sample to heat up or vibrate. These changes in temperature or pressure generate sound waves that can be detected using an ultrasound transducer. The resulting image shows the distribution of light absorption or scattering in the sample.\n",
      "\n",
      "Optoacoustic techniques have been used in a variety of applications, including\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Tell what is optoacosutic.\"\n",
    ")\n",
    "\n",
    "input = tokenizer(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "        \n",
    "    output_ids = model.generate(\n",
    "        **input,\n",
    "        max_new_tokens = 200,\n",
    "        do_sample = True,\n",
    "        temperature = 0.7,\n",
    "        top_p = 0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_time=20,\n",
    "    )\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "examples = [\n",
    "    {\n",
    "        \"instruction\": \"Explain overfitting in machine learning.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Overfitting happens when a model memorizes the training data instead of learning general patterns, so it performs well on training data but poorly on new, unseen data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Give me 3 tips to study more effectively.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"1) Study in focused blocks with breaks, 2) test yourself instead of only rereading, 3) sleep well to consolidate memory.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what a GPU is to a 10-year-old.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A GPU is like a super helper in your computer that can do many small math problems at the same time, which makes games and AI run faster.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "raw_dataset = Dataset.from_list(examples)\n",
    "\n",
    "def format_example(example):\n",
    "    instruction = example['instruction']\n",
    "    if example.get(\"input\"):\n",
    "        prompt = f\"Instruction: {instruction}\\nInput: {example['input']}\\nResponse:\"\n",
    "    else:\n",
    "        prompt = f\"instruction: {instruction}\\nResponse:\"\n",
    "    prompt = f\"{prompt} {example['output']}\"\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length= 200,\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "dataset_ids = raw_dataset.map(format_example)\n",
    "print(dataset_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0312ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha= 16,\n",
    "    lora_dropout= 0.05,\n",
    "    target_modules= ['q_proj', 'k_proj'],\n",
    "    task_type= 'CAUSAL_LM'\n",
    ")\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koorosh/Documents/Coding/Foundation_Model/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:02, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.321900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=2.3835535645484924, metrics={'train_runtime': 3.5043, 'train_samples_per_second': 3.424, 'train_steps_per_second': 1.141, 'total_flos': 24118890332160.0, 'train_loss': 2.3835535645484924, 'epoch': 4.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "output_dir = \"./llama-2-7b-lora-demo\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= output_dir,\n",
    "    per_device_train_batch_size= 1,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    num_train_epochs= 4,\n",
    "    learning_rate= 2e-4,\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],  # no wandb or anything\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model= model_lora,\n",
    "    args= training_args,\n",
    "    train_dataset= dataset_ids\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
